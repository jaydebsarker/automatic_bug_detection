{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f86b86f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device:/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import ktrain\n",
    "\n",
    "if tf.test.gpu_device_name(): \n",
    "\n",
    "    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50743ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import ktrain\n",
    "from ktrain import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3adab8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "num_folds = 5\n",
    "\n",
    "run_precision = []\n",
    "run_recall = []\n",
    "run_f1score = []\n",
    "run_accuracy = []\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=False, random_state=125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c240016",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('sstubsIn_edited.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce99cbbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10231, 5)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75d95d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"pre-processing train data...\")\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from tqdm import tqdm\n",
    "\n",
    "#label_names = [\"bugType\"]\n",
    "#y_train = data[label_names].values\n",
    "\n",
    "raw_docs_train = data['sourceBeforeFix'].tolist()\n",
    "#raw_docs_test = test_data['message'].tolist() \n",
    "num_classes = 4\n",
    "\n",
    "#processed_docs_train = []\n",
    "#for doc in tqdm(raw_docs_train):\n",
    "#    tokens = word_tokenize(doc)\n",
    "#    filtered = [word for word in tokens if word not in stop_words]\n",
    "#    processed_docs_train.append(\" \".join(filtered))\n",
    "#end for\n",
    "'''\n",
    "processed_docs_test = []\n",
    "for doc in tqdm(raw_docs_test):\n",
    "    tokens = word_tokenize(doc)\n",
    "    filtered = [word for word in tokens if word not in stop_words]\n",
    "    processed_docs_test.append(\" \".join(filtered))\n",
    "#end for\n",
    "'''\n",
    "\n",
    "\n",
    "#print(\"tokenizing input data...\")\n",
    "#tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\n",
    "#tokenizer.fit_on_texts(processed_docs_train )  #leaky\n",
    "#word_seq_train = tokenizer.texts_to_sequences(processed_docs_train)\n",
    "#word_seq_test = tokenizer.texts_to_sequences(processed_docs_test)\n",
    "#word_index = tokenizer.word_index\n",
    "#print(\"dictionary size: \", len(word_index))\n",
    "\n",
    "#pad sequences\n",
    "#word_seq_train = sequence.pad_sequences(word_seq_train, maxlen=max_seq_len)\n",
    "word_seq_train = raw_docs_train\n",
    "word_seq_train = np.array(word_seq_train)\n",
    "\n",
    "#data.bugType.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "434bbc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [0,1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1463cf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = {\n",
    "    'CHANGE_IDENTIFIER': 0, \n",
    "    'CHANGE_MODIFIER': 0,\n",
    "    'CHANGE_NUMERAL': 0, \n",
    "    'SWAP_BOOLEAN_LITERAL': 0,\n",
    "     'DIFFERENT_METHOD_SAME_ARGS': 1, \n",
    "     'OVERLOAD_METHOD_MORE_ARGS': 1, \n",
    "    'OVERLOAD_METHOD_DELETED_ARGS': 1,\n",
    "     'CHANGE_CALLER_IN_FUNCTION_CALL': 1, \n",
    "     'SWAP_ARGUMENTS': 1,\n",
    "     'CHANGE_OPERATOR': 2, \n",
    "    'CHANGE_UNARY_OPERATOR': 2, \n",
    "    'CHANGE_OPERAND': 2, \n",
    "     'LESS_SPECIFIC_IF': 3,\n",
    "    'MORE_SPECIFIC_IF': 3,\n",
    "     'ADD_THROWS_EXCEPTION': 3, \n",
    "      'DELETE_THROWS_EXCEPTION':3,\n",
    "       \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cff52e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_initial= data['bugType'].values\n",
    "\n",
    "\n",
    "y_train_initial = [encoding[x] for x in y_train_initial]\n",
    "y_train_initial=np.array(y_train_initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "424b03a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold  1\n",
      "language: en\n",
      "Word Counts: 7750\n",
      "Nrows: 7274\n",
      "7274 train sequences\n",
      "train sequence lengths:\n",
      "\tmean : 5\n",
      "\t95percentile : 12\n",
      "\t99percentile : 45\n",
      "x_train shape: (7274,150)\n",
      "y_train shape: (7274, 4)\n",
      "Is Multi-Label? False\n",
      "2047 test sequences\n",
      "test sequence lengths:\n",
      "\tmean : 2\n",
      "\t95percentile : 7\n",
      "\t99percentile : 15\n",
      "x_test shape: (2047,150)\n",
      "y_test shape: (2047, 4)\n",
      "task: text classification\n",
      "Is Multi-Label? False\n",
      "compiling word ID features...\n",
      "maxlen is 150\n",
      "word vectors will be loaded from: https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
      "processing pretrained word vectors...\n",
      "downloading pretrained word vectors to /home/amiangshu/ktrain_data ...\n",
      "[██████████████████████████████████████████████████]\n",
      "extracting pretrained word vectors...\n",
      "done.\n",
      "\n",
      "cleanup downloaded zip...\n",
      "done.\n",
      "\n",
      "loading pretrained word vectors...this may take a few moments...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "early_stopping automatically enabled at patience=5\n",
      "reduce_on_plateau automatically enabled at patience=2\n",
      "\n",
      "\n",
      "begin training using triangular learning rate policy with max lr of 0.01...\n",
      "Epoch 1/1024\n",
      "728/728 [==============================] - 16s 19ms/step - loss: 0.8736 - accuracy: 0.6277 - val_loss: 0.7486 - val_accuracy: 0.6541\n",
      "Epoch 2/1024\n",
      "728/728 [==============================] - 13s 18ms/step - loss: 0.6718 - accuracy: 0.6908 - val_loss: 0.7787 - val_accuracy: 0.6546\n",
      "Epoch 3/1024\n",
      "728/728 [==============================] - 12s 17ms/step - loss: 0.6605 - accuracy: 0.6815 - val_loss: 0.8569 - val_accuracy: 0.6556\n",
      "\n",
      "Epoch 00003: Reducing Max LR on Plateau: new max lr will be 0.005 (if not early_stopping).\n",
      "Epoch 4/1024\n",
      "728/728 [==============================] - 13s 18ms/step - loss: 0.6141 - accuracy: 0.7084 - val_loss: 0.8431 - val_accuracy: 0.6571\n",
      "Epoch 5/1024\n",
      "728/728 [==============================] - 12s 17ms/step - loss: 0.5682 - accuracy: 0.7167 - val_loss: 0.8390 - val_accuracy: 0.6468\n",
      "\n",
      "Epoch 00005: Reducing Max LR on Plateau: new max lr will be 0.0025 (if not early_stopping).\n",
      "Epoch 6/1024\n",
      "728/728 [==============================] - 12s 17ms/step - loss: 0.5335 - accuracy: 0.7338 - val_loss: 0.8792 - val_accuracy: 0.6517\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Weights from best epoch have been loaded into model.\n",
      "language: en\n",
      "Word Counts: 2291\n",
      "Nrows: 910\n",
      "910 train sequences\n",
      "train sequence lengths:\n",
      "\tmean : 7\n",
      "\t95percentile : 11\n",
      "\t99percentile : 57\n",
      "x_train shape: (910,150)\n",
      "y_train shape: (910, 4)\n",
      "Is Multi-Label? False\n",
      "910 test sequences\n",
      "test sequence lengths:\n",
      "\tmean : 7\n",
      "\t95percentile : 11\n",
      "\t99percentile : 57\n",
      "x_test shape: (910,150)\n",
      "y_test shape: (910, 4)\n",
      "task: text classification\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.83      0.73       563\n",
      "           1       0.32      0.24      0.27       244\n",
      "           2       0.00      0.00      0.00        53\n",
      "           3       0.00      0.00      0.00        50\n",
      "\n",
      "    accuracy                           0.58       910\n",
      "   macro avg       0.24      0.27      0.25       910\n",
      "weighted avg       0.49      0.58      0.52       910\n",
      "\n",
      "\n",
      "Fold  2\n",
      "language: en\n",
      "Word Counts: 7739\n",
      "Nrows: 7275\n",
      "7275 train sequences\n",
      "train sequence lengths:\n",
      "\tmean : 6\n",
      "\t95percentile : 12\n",
      "\t99percentile : 49\n",
      "x_train shape: (7275,150)\n",
      "y_train shape: (7275, 4)\n",
      "Is Multi-Label? False\n",
      "2046 test sequences\n",
      "test sequence lengths:\n",
      "\tmean : 2\n",
      "\t95percentile : 7\n",
      "\t99percentile : 17\n",
      "x_test shape: (2046,150)\n",
      "y_test shape: (2046, 4)\n",
      "task: text classification\n",
      "Is Multi-Label? False\n",
      "compiling word ID features...\n",
      "maxlen is 150\n",
      "word vectors will be loaded from: https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
      "processing pretrained word vectors...\n",
      "loading pretrained word vectors...this may take a few moments...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "early_stopping automatically enabled at patience=5\n",
      "reduce_on_plateau automatically enabled at patience=2\n",
      "\n",
      "\n",
      "begin training using triangular learning rate policy with max lr of 0.01...\n",
      "Epoch 1/1024\n",
      "728/728 [==============================] - 16s 20ms/step - loss: 0.8903 - accuracy: 0.6108 - val_loss: 0.6706 - val_accuracy: 0.7048\n",
      "Epoch 2/1024\n",
      "728/728 [==============================] - 15s 21ms/step - loss: 0.7088 - accuracy: 0.6776 - val_loss: 0.6843 - val_accuracy: 0.7058\n",
      "Epoch 3/1024\n",
      "728/728 [==============================] - 14s 19ms/step - loss: 0.7183 - accuracy: 0.6745 - val_loss: 0.7361 - val_accuracy: 0.6794\n",
      "\n",
      "Epoch 00003: Reducing Max LR on Plateau: new max lr will be 0.005 (if not early_stopping).\n",
      "Epoch 4/1024\n",
      "728/728 [==============================] - 14s 19ms/step - loss: 0.6183 - accuracy: 0.7025 - val_loss: 0.7066 - val_accuracy: 0.6960\n",
      "Epoch 5/1024\n",
      "728/728 [==============================] - 15s 21ms/step - loss: 0.6041 - accuracy: 0.7024 - val_loss: 0.7440 - val_accuracy: 0.6730\n",
      "\n",
      "Epoch 00005: Reducing Max LR on Plateau: new max lr will be 0.0025 (if not early_stopping).\n",
      "Epoch 6/1024\n",
      "728/728 [==============================] - 14s 20ms/step - loss: 0.5619 - accuracy: 0.7227 - val_loss: 0.7500 - val_accuracy: 0.6906\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Weights from best epoch have been loaded into model.\n",
      "language: en\n",
      "Word Counts: 2277\n",
      "Nrows: 910\n",
      "910 train sequences\n",
      "train sequence lengths:\n",
      "\tmean : 5\n",
      "\t95percentile : 11\n",
      "\t99percentile : 58\n",
      "x_train shape: (910,150)\n",
      "y_train shape: (910, 4)\n",
      "Is Multi-Label? False\n",
      "910 test sequences\n",
      "test sequence lengths:\n",
      "\tmean : 5\n",
      "\t95percentile : 11\n",
      "\t99percentile : 58\n",
      "x_test shape: (910,150)\n",
      "y_test shape: (910, 4)\n",
      "task: text classification\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.79      0.72       572\n",
      "           1       0.40      0.31      0.35       250\n",
      "           2       0.04      0.02      0.03        46\n",
      "           3       0.00      0.00      0.00        42\n",
      "\n",
      "    accuracy                           0.58       910\n",
      "   macro avg       0.27      0.28      0.27       910\n",
      "weighted avg       0.52      0.58      0.55       910\n",
      "\n",
      "\n",
      "Fold  3\n",
      "language: en\n",
      "Word Counts: 6573\n",
      "Nrows: 7275\n",
      "7275 train sequences\n",
      "train sequence lengths:\n",
      "\tmean : 4\n",
      "\t95percentile : 10\n",
      "\t99percentile : 24\n",
      "x_train shape: (7275,150)\n",
      "y_train shape: (7275, 4)\n",
      "Is Multi-Label? False\n",
      "2046 test sequences\n",
      "test sequence lengths:\n",
      "\tmean : 6\n",
      "\t95percentile : 15\n",
      "\t99percentile : 66\n",
      "x_test shape: (2046,150)\n",
      "y_test shape: (2046, 4)\n",
      "task: text classification\n",
      "Is Multi-Label? False\n",
      "compiling word ID features...\n",
      "maxlen is 150\n",
      "word vectors will be loaded from: https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
      "processing pretrained word vectors...\n",
      "loading pretrained word vectors...this may take a few moments...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "early_stopping automatically enabled at patience=5\n",
      "reduce_on_plateau automatically enabled at patience=2\n",
      "\n",
      "\n",
      "begin training using triangular learning rate policy with max lr of 0.01...\n",
      "Epoch 1/1024\n",
      "728/728 [==============================] - 31s 40ms/step - loss: 0.8625 - accuracy: 0.6255 - val_loss: 0.8575 - val_accuracy: 0.6388\n",
      "Epoch 2/1024\n",
      "728/728 [==============================] - 29s 39ms/step - loss: 0.6490 - accuracy: 0.7005 - val_loss: 0.8620 - val_accuracy: 0.6256\n",
      "Epoch 3/1024\n",
      "728/728 [==============================] - 32s 44ms/step - loss: 0.6099 - accuracy: 0.7133 - val_loss: 0.8608 - val_accuracy: 0.6300\n",
      "\n",
      "Epoch 00003: Reducing Max LR on Plateau: new max lr will be 0.005 (if not early_stopping).\n",
      "Epoch 4/1024\n",
      "728/728 [==============================] - 31s 43ms/step - loss: 0.5749 - accuracy: 0.7300 - val_loss: 0.9592 - val_accuracy: 0.6227\n",
      "Epoch 5/1024\n",
      "728/728 [==============================] - 29s 40ms/step - loss: 0.5516 - accuracy: 0.7289 - val_loss: 0.9775 - val_accuracy: 0.6241\n",
      "\n",
      "Epoch 00005: Reducing Max LR on Plateau: new max lr will be 0.0025 (if not early_stopping).\n",
      "Epoch 6/1024\n",
      "728/728 [==============================] - 31s 43ms/step - loss: 0.4975 - accuracy: 0.7520 - val_loss: 0.9991 - val_accuracy: 0.6359\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Weights from best epoch have been loaded into model.\n",
      "language: en\n",
      "Word Counts: 2214\n",
      "Nrows: 910\n",
      "910 train sequences\n",
      "train sequence lengths:\n",
      "\tmean : 5\n",
      "\t95percentile : 10\n",
      "\t99percentile : 31\n",
      "x_train shape: (910,150)\n",
      "y_train shape: (910, 4)\n",
      "Is Multi-Label? False\n",
      "910 test sequences\n",
      "test sequence lengths:\n",
      "\tmean : 5\n",
      "\t95percentile : 10\n",
      "\t99percentile : 31\n",
      "x_test shape: (910,150)\n",
      "y_test shape: (910, 4)\n",
      "task: text classification\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.73      0.65       585\n",
      "           1       0.10      0.07      0.08       235\n",
      "           2       0.18      0.04      0.07        46\n",
      "           3       0.11      0.02      0.04        44\n",
      "\n",
      "    accuracy                           0.49       910\n",
      "   macro avg       0.25      0.22      0.21       910\n",
      "weighted avg       0.42      0.49      0.45       910\n",
      "\n",
      "\n",
      "Fold  4\n",
      "language: en\n",
      "Word Counts: 7130\n",
      "Nrows: 7275\n",
      "7275 train sequences\n",
      "train sequence lengths:\n",
      "\tmean : 5\n",
      "\t95percentile : 12\n",
      "\t99percentile : 41\n",
      "x_train shape: (7275,150)\n",
      "y_train shape: (7275, 4)\n",
      "Is Multi-Label? False\n",
      "2046 test sequences\n",
      "test sequence lengths:\n",
      "\tmean : 4\n",
      "\t95percentile : 7\n",
      "\t99percentile : 26\n",
      "x_test shape: (2046,150)\n",
      "y_test shape: (2046, 4)\n",
      "task: text classification\n",
      "Is Multi-Label? False\n",
      "compiling word ID features...\n",
      "maxlen is 150\n",
      "word vectors will be loaded from: https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
      "processing pretrained word vectors...\n",
      "loading pretrained word vectors...this may take a few moments...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "early_stopping automatically enabled at patience=5\n",
      "reduce_on_plateau automatically enabled at patience=2\n",
      "\n",
      "\n",
      "begin training using triangular learning rate policy with max lr of 0.01...\n",
      "Epoch 1/1024\n",
      "728/728 [==============================] - 16s 19ms/step - loss: 0.8505 - accuracy: 0.6307 - val_loss: 0.8726 - val_accuracy: 0.6197\n",
      "Epoch 2/1024\n",
      "728/728 [==============================] - 15s 20ms/step - loss: 0.6875 - accuracy: 0.6901 - val_loss: 0.9260 - val_accuracy: 0.5919\n",
      "Epoch 3/1024\n",
      "728/728 [==============================] - 14s 20ms/step - loss: 0.6999 - accuracy: 0.6906 - val_loss: 0.9542 - val_accuracy: 0.5909\n",
      "\n",
      "Epoch 00003: Reducing Max LR on Plateau: new max lr will be 0.005 (if not early_stopping).\n",
      "Epoch 4/1024\n",
      "728/728 [==============================] - 14s 19ms/step - loss: 0.5899 - accuracy: 0.7203 - val_loss: 0.9870 - val_accuracy: 0.5992\n",
      "Epoch 5/1024\n",
      "728/728 [==============================] - 15s 21ms/step - loss: 0.5813 - accuracy: 0.7158 - val_loss: 0.9922 - val_accuracy: 0.5934\n",
      "\n",
      "Epoch 00005: Reducing Max LR on Plateau: new max lr will be 0.0025 (if not early_stopping).\n",
      "Epoch 6/1024\n",
      "728/728 [==============================] - 14s 19ms/step - loss: 0.5311 - accuracy: 0.7452 - val_loss: 1.0201 - val_accuracy: 0.5973\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Weights from best epoch have been loaded into model.\n",
      "language: en\n",
      "Word Counts: 1949\n",
      "Nrows: 910\n",
      "910 train sequences\n",
      "train sequence lengths:\n",
      "\tmean : 5\n",
      "\t95percentile : 11\n",
      "\t99percentile : 58\n",
      "x_train shape: (910,150)\n",
      "y_train shape: (910, 4)\n",
      "Is Multi-Label? False\n",
      "910 test sequences\n",
      "test sequence lengths:\n",
      "\tmean : 5\n",
      "\t95percentile : 11\n",
      "\t99percentile : 58\n",
      "x_test shape: (910,150)\n",
      "y_test shape: (910, 4)\n",
      "task: text classification\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amiangshu/anaconda3/envs/dsaktrain/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.90      0.77       587\n",
      "           1       0.36      0.16      0.23       237\n",
      "           2       0.20      0.08      0.12        36\n",
      "           3       0.00      0.00      0.00        50\n",
      "\n",
      "    accuracy                           0.63       910\n",
      "   macro avg       0.31      0.29      0.28       910\n",
      "weighted avg       0.54      0.63      0.56       910\n",
      "\n",
      "\n",
      "Fold  5\n",
      "language: en\n",
      "Word Counts: 7506\n",
      "Nrows: 7275\n",
      "7275 train sequences\n",
      "train sequence lengths:\n",
      "\tmean : 5\n",
      "\t95percentile : 11\n",
      "\t99percentile : 47\n",
      "x_train shape: (7275,150)\n",
      "y_train shape: (7275, 4)\n",
      "Is Multi-Label? False\n",
      "2046 test sequences\n",
      "test sequence lengths:\n",
      "\tmean : 3\n",
      "\t95percentile : 8\n",
      "\t99percentile : 17\n",
      "x_test shape: (2046,150)\n",
      "y_test shape: (2046, 4)\n",
      "task: text classification\n",
      "Is Multi-Label? False\n",
      "compiling word ID features...\n",
      "maxlen is 150\n",
      "word vectors will be loaded from: https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
      "processing pretrained word vectors...\n",
      "loading pretrained word vectors...this may take a few moments...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "early_stopping automatically enabled at patience=5\n",
      "reduce_on_plateau automatically enabled at patience=2\n",
      "\n",
      "\n",
      "begin training using triangular learning rate policy with max lr of 0.01...\n",
      "Epoch 1/1024\n",
      "728/728 [==============================] - 16s 20ms/step - loss: 0.8563 - accuracy: 0.6384 - val_loss: 0.8135 - val_accuracy: 0.6124\n",
      "Epoch 2/1024\n",
      "728/728 [==============================] - 14s 19ms/step - loss: 0.6747 - accuracy: 0.6841 - val_loss: 0.8248 - val_accuracy: 0.6056\n",
      "Epoch 3/1024\n",
      "728/728 [==============================] - 14s 19ms/step - loss: 0.6362 - accuracy: 0.7081 - val_loss: 0.8638 - val_accuracy: 0.6134\n",
      "\n",
      "Epoch 00003: Reducing Max LR on Plateau: new max lr will be 0.005 (if not early_stopping).\n",
      "Epoch 4/1024\n",
      "728/728 [==============================] - 14s 19ms/step - loss: 0.6167 - accuracy: 0.7059 - val_loss: 0.8554 - val_accuracy: 0.6080\n",
      "Epoch 5/1024\n",
      "728/728 [==============================] - 14s 19ms/step - loss: 0.5607 - accuracy: 0.7345 - val_loss: 0.8746 - val_accuracy: 0.5997\n",
      "\n",
      "Epoch 00005: Reducing Max LR on Plateau: new max lr will be 0.0025 (if not early_stopping).\n",
      "Epoch 6/1024\n",
      "728/728 [==============================] - 14s 19ms/step - loss: 0.5345 - accuracy: 0.7379 - val_loss: 0.8713 - val_accuracy: 0.6075\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Weights from best epoch have been loaded into model.\n",
      "language: en\n",
      "Word Counts: 2237\n",
      "Nrows: 910\n",
      "910 train sequences\n",
      "train sequence lengths:\n",
      "\tmean : 5\n",
      "\t95percentile : 11\n",
      "\t99percentile : 58\n",
      "x_train shape: (910,150)\n",
      "y_train shape: (910, 4)\n",
      "Is Multi-Label? False\n",
      "910 test sequences\n",
      "test sequence lengths:\n",
      "\tmean : 5\n",
      "\t95percentile : 11\n",
      "\t99percentile : 58\n",
      "x_test shape: (910,150)\n",
      "y_test shape: (910, 4)\n",
      "task: text classification\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.92      0.78       599\n",
      "           1       0.40      0.14      0.21       221\n",
      "           2       0.00      0.00      0.00        53\n",
      "           3       0.00      0.00      0.00        37\n",
      "\n",
      "    accuracy                           0.64       910\n",
      "   macro avg       0.27      0.27      0.25       910\n",
      "weighted avg       0.54      0.64      0.56       910\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "count = 1\n",
    "\n",
    "for train_index, test_index in kf.split(word_seq_train):\n",
    "    x_trn, x_tst = word_seq_train[train_index], word_seq_train[test_index]\n",
    "    y_trn, y_tst = y_train_initial[train_index], y_train_initial[test_index]\n",
    "    \n",
    "    x_new_train, x_val, y_new_train, y_val= train_test_split(x_trn, y_trn, test_size=0.11115, random_state=125)\n",
    "    \n",
    "    print(\"\\nFold \", count)\n",
    "    \n",
    "    (x_train,  y_train), (x_test, y_test),  preproc = text.texts_from_array(x_train=x_new_train, y_train=y_new_train,\n",
    "                                                                      x_test=x_tst, y_test=y_tst,\n",
    "                                                                       class_names=class_names,\n",
    "                                                                       preprocess_mode='standard',\n",
    "                                                                       maxlen=150, \n",
    "                                                                       max_features=1000)\n",
    "    \n",
    "    model = text.text_classifier('bigru', train_data=(x_train, y_train), preproc=preproc)\n",
    "    \n",
    "    learner = ktrain.get_learner(model, train_data=(x_train, y_train), \n",
    "                             val_data=(x_test, y_test),\n",
    "                             batch_size=10)\n",
    "    \n",
    "    \n",
    "    callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1)\n",
    "    #learner.lr_find()\n",
    "\n",
    "    \n",
    "    #learner.fit_onecycle(2e-5, 40, callbacks=[callbacks])\n",
    "\n",
    "    learner.autofit(0.01)\n",
    "\n",
    "    \n",
    "    #predictor = ktrain.get_predictor(learner.model, preproc)\n",
    "    #predictor.get_classes()\n",
    "    \n",
    "    (x_train,  y_train), (x_test, y_test),  preproc = text.texts_from_array(x_train=x_val, y_train=y_val,\n",
    "                                                                       x_test=x_val, y_test=y_val,\n",
    "                                                                       \n",
    "                                                                       class_names=class_names,\n",
    "                                                                       preprocess_mode='standard',\n",
    "                                                                       maxlen=150, \n",
    "                                                                       max_features=1000)\n",
    "    \n",
    "    learner.validate(val_data=(x_test, y_test), class_names=class_names)\n",
    "    \n",
    "    '''\n",
    "    print(metrics.classification_report(y_tst, y_pred))\n",
    "    \n",
    "    precision = precision_score(y_tst, y_pred, pos_label=1)\n",
    "    recall = recall_score(y_tst, y_pred, pos_label=1)\n",
    "    f1score = f1_score(y_tst, y_pred, pos_label=1)\n",
    "    lstm_accuracy = accuracy_score(y_tst, y_pred)\n",
    "\n",
    "    run_accuracy.append(accuracy)\n",
    "    run_f1score.append(f1score)\n",
    "    run_precision.append(precision)\n",
    "    run_recall.append(recall)\n",
    "    '''\n",
    "    count = count+1\n",
    "    #X_train = data['sourceBeforeFix'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8ec683",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ee91f77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bcafaaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a439476",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc6056a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
