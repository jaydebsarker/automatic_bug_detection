{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f86b86f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device:/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import ktrain\n",
    "\n",
    "if tf.test.gpu_device_name(): \n",
    "\n",
    "    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50743ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import ktrain\n",
    "from ktrain import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3adab8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "num_folds = 5\n",
    "\n",
    "run_precision = []\n",
    "run_recall = []\n",
    "run_f1score = []\n",
    "run_accuracy = []\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=False, random_state=125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c240016",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('sstubsIn_edited.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce99cbbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10231, 5)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75d95d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"pre-processing train data...\")\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from tqdm import tqdm\n",
    "\n",
    "#label_names = [\"bugType\"]\n",
    "#y_train = data[label_names].values\n",
    "\n",
    "raw_docs_train = data['sourceBeforeFix'].tolist()\n",
    "#raw_docs_test = test_data['message'].tolist() \n",
    "num_classes = 4\n",
    "\n",
    "#processed_docs_train = []\n",
    "#for doc in tqdm(raw_docs_train):\n",
    "#    tokens = word_tokenize(doc)\n",
    "#    filtered = [word for word in tokens if word not in stop_words]\n",
    "#    processed_docs_train.append(\" \".join(filtered))\n",
    "#end for\n",
    "'''\n",
    "processed_docs_test = []\n",
    "for doc in tqdm(raw_docs_test):\n",
    "    tokens = word_tokenize(doc)\n",
    "    filtered = [word for word in tokens if word not in stop_words]\n",
    "    processed_docs_test.append(\" \".join(filtered))\n",
    "#end for\n",
    "'''\n",
    "\n",
    "\n",
    "#print(\"tokenizing input data...\")\n",
    "#tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\n",
    "#tokenizer.fit_on_texts(processed_docs_train )  #leaky\n",
    "#word_seq_train = tokenizer.texts_to_sequences(processed_docs_train)\n",
    "#word_seq_test = tokenizer.texts_to_sequences(processed_docs_test)\n",
    "#word_index = tokenizer.word_index\n",
    "#print(\"dictionary size: \", len(word_index))\n",
    "\n",
    "#pad sequences\n",
    "#word_seq_train = sequence.pad_sequences(word_seq_train, maxlen=max_seq_len)\n",
    "word_seq_train = raw_docs_train\n",
    "word_seq_train = np.array(word_seq_train)\n",
    "\n",
    "#data.bugType.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "434bbc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [0,1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1463cf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = {\n",
    "    'CHANGE_IDENTIFIER': 0, \n",
    "    'CHANGE_MODIFIER': 0,\n",
    "    'CHANGE_NUMERAL': 0, \n",
    "    'SWAP_BOOLEAN_LITERAL': 0,\n",
    "     'DIFFERENT_METHOD_SAME_ARGS': 1, \n",
    "     'OVERLOAD_METHOD_MORE_ARGS': 1, \n",
    "    'OVERLOAD_METHOD_DELETED_ARGS': 1,\n",
    "     'CHANGE_CALLER_IN_FUNCTION_CALL': 1, \n",
    "     'SWAP_ARGUMENTS': 1,\n",
    "     'CHANGE_OPERATOR': 2, \n",
    "    'CHANGE_UNARY_OPERATOR': 2, \n",
    "    'CHANGE_OPERAND': 2, \n",
    "     'LESS_SPECIFIC_IF': 3,\n",
    "    'MORE_SPECIFIC_IF': 3,\n",
    "     'ADD_THROWS_EXCEPTION': 3, \n",
    "      'DELETE_THROWS_EXCEPTION':3,\n",
    "       \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cff52e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_initial= data['bugType'].values\n",
    "\n",
    "\n",
    "y_train_initial = [encoding[x] for x in y_train_initial]\n",
    "y_train_initial=np.array(y_train_initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3862044a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 1 ... 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_train_initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e906b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0 6423]\n",
      " [   1 2737]\n",
      " [   2  565]\n",
      " [   3  506]]\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(y_train_initial, return_counts=True)\n",
    "\n",
    "print(np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "424b03a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold  1\n",
      "language: en\n",
      "Word Counts: 7750\n",
      "Nrows: 7274\n",
      "7274 train sequences\n",
      "train sequence lengths:\n",
      "\tmean : 5\n",
      "\t95percentile : 12\n",
      "\t99percentile : 45\n",
      "x_train shape: (7274,150)\n",
      "y_train shape: (7274, 4)\n",
      "Is Multi-Label? False\n",
      "2047 test sequences\n",
      "test sequence lengths:\n",
      "\tmean : 2\n",
      "\t95percentile : 7\n",
      "\t99percentile : 15\n",
      "x_test shape: (2047,150)\n",
      "y_test shape: (2047, 4)\n",
      "task: text classification\n",
      "Is Multi-Label? False\n",
      "compiling word ID features...\n",
      "maxlen is 150\n",
      "building document-term matrix... this may take a few moments...\n",
      "rows: 1-7274\n",
      "computing log-count ratios...\n",
      "done.\n",
      "early_stopping automatically enabled at patience=5\n",
      "reduce_on_plateau automatically enabled at patience=2\n",
      "\n",
      "\n",
      "begin training using triangular learning rate policy with max lr of 0.01...\n",
      "Epoch 1/1024\n",
      "728/728 [==============================] - 3s 3ms/step - loss: 1.1943 - accuracy: 0.4983 - val_loss: 0.9597 - val_accuracy: 0.6072\n",
      "Epoch 2/1024\n",
      "728/728 [==============================] - 2s 3ms/step - loss: 0.9218 - accuracy: 0.6080 - val_loss: 0.9564 - val_accuracy: 0.6028\n",
      "Epoch 3/1024\n",
      "728/728 [==============================] - 2s 3ms/step - loss: 0.8866 - accuracy: 0.6035 - val_loss: 0.9677 - val_accuracy: 0.6004\n",
      "Epoch 4/1024\n",
      "728/728 [==============================] - 3s 4ms/step - loss: 0.8506 - accuracy: 0.6099 - val_loss: 0.9749 - val_accuracy: 0.6072\n",
      "\n",
      "Epoch 00004: Reducing Max LR on Plateau: new max lr will be 0.005 (if not early_stopping).\n",
      "Epoch 5/1024\n",
      "728/728 [==============================] - 2s 3ms/step - loss: 0.8405 - accuracy: 0.6215 - val_loss: 0.9768 - val_accuracy: 0.6067\n",
      "Epoch 6/1024\n",
      "728/728 [==============================] - 2s 3ms/step - loss: 0.8316 - accuracy: 0.6267 - val_loss: 0.9889 - val_accuracy: 0.6023\n",
      "\n",
      "Epoch 00006: Reducing Max LR on Plateau: new max lr will be 0.0025 (if not early_stopping).\n",
      "Epoch 7/1024\n",
      "728/728 [==============================] - 2s 3ms/step - loss: 0.8404 - accuracy: 0.6142 - val_loss: 0.9914 - val_accuracy: 0.6038\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Weights from best epoch have been loaded into model.\n",
      "language: en\n",
      "Word Counts: 2291\n",
      "Nrows: 910\n",
      "910 train sequences\n",
      "train sequence lengths:\n",
      "\tmean : 7\n",
      "\t95percentile : 11\n",
      "\t99percentile : 57\n",
      "x_train shape: (910,150)\n",
      "y_train shape: (910, 4)\n",
      "Is Multi-Label? False\n",
      "910 test sequences\n",
      "test sequence lengths:\n",
      "\tmean : 7\n",
      "\t95percentile : 11\n",
      "\t99percentile : 57\n",
      "x_test shape: (910,150)\n",
      "y_test shape: (910, 4)\n",
      "task: text classification\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.58      0.61       563\n",
      "           1       0.31      0.32      0.31       244\n",
      "           2       0.07      0.06      0.06        53\n",
      "           3       0.07      0.16      0.10        50\n",
      "\n",
      "    accuracy                           0.46       910\n",
      "   macro avg       0.27      0.28      0.27       910\n",
      "weighted avg       0.49      0.46      0.47       910\n",
      "\n",
      "\n",
      "Fold  2\n",
      "language: en\n",
      "Word Counts: 7739\n",
      "Nrows: 7275\n",
      "7275 train sequences\n",
      "train sequence lengths:\n",
      "\tmean : 6\n",
      "\t95percentile : 12\n",
      "\t99percentile : 49\n",
      "x_train shape: (7275,150)\n",
      "y_train shape: (7275, 4)\n",
      "Is Multi-Label? False\n",
      "2046 test sequences\n",
      "test sequence lengths:\n",
      "\tmean : 2\n",
      "\t95percentile : 7\n",
      "\t99percentile : 17\n",
      "x_test shape: (2046,150)\n",
      "y_test shape: (2046, 4)\n",
      "task: text classification\n",
      "Is Multi-Label? False\n",
      "compiling word ID features...\n",
      "maxlen is 150\n",
      "building document-term matrix... this may take a few moments...\n",
      "rows: 1-7275\n",
      "computing log-count ratios...\n",
      "done.\n",
      "early_stopping automatically enabled at patience=5\n",
      "reduce_on_plateau automatically enabled at patience=2\n",
      "\n",
      "\n",
      "begin training using triangular learning rate policy with max lr of 0.01...\n",
      "Epoch 1/1024\n",
      "728/728 [==============================] - 3s 4ms/step - loss: 1.2067 - accuracy: 0.5266 - val_loss: 0.8789 - val_accuracy: 0.6691\n",
      "Epoch 2/1024\n",
      "728/728 [==============================] - 2s 3ms/step - loss: 0.9200 - accuracy: 0.6449 - val_loss: 0.8642 - val_accuracy: 0.6608\n",
      "Epoch 3/1024\n",
      "728/728 [==============================] - 2s 3ms/step - loss: 0.8804 - accuracy: 0.6485 - val_loss: 0.8684 - val_accuracy: 0.6652\n",
      "Epoch 4/1024\n",
      "728/728 [==============================] - 2s 3ms/step - loss: 0.8645 - accuracy: 0.6481 - val_loss: 0.8735 - val_accuracy: 0.6628\n",
      "\n",
      "Epoch 00004: Reducing Max LR on Plateau: new max lr will be 0.005 (if not early_stopping).\n",
      "Epoch 5/1024\n",
      "728/728 [==============================] - 3s 4ms/step - loss: 0.8301 - accuracy: 0.6669 - val_loss: 0.8735 - val_accuracy: 0.6593\n",
      "Epoch 6/1024\n",
      "728/728 [==============================] - 3s 4ms/step - loss: 0.8327 - accuracy: 0.6598 - val_loss: 0.8777 - val_accuracy: 0.6598\n",
      "\n",
      "Epoch 00006: Reducing Max LR on Plateau: new max lr will be 0.0025 (if not early_stopping).\n",
      "Epoch 7/1024\n",
      "728/728 [==============================] - 2s 3ms/step - loss: 0.8325 - accuracy: 0.6519 - val_loss: 0.8795 - val_accuracy: 0.6588\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Weights from best epoch have been loaded into model.\n",
      "language: en\n",
      "Word Counts: 2277\n",
      "Nrows: 910\n",
      "910 train sequences\n",
      "train sequence lengths:\n",
      "\tmean : 5\n",
      "\t95percentile : 11\n",
      "\t99percentile : 58\n",
      "x_train shape: (910,150)\n",
      "y_train shape: (910, 4)\n",
      "Is Multi-Label? False\n",
      "910 test sequences\n",
      "test sequence lengths:\n",
      "\tmean : 5\n",
      "\t95percentile : 11\n",
      "\t99percentile : 58\n",
      "x_test shape: (910,150)\n",
      "y_test shape: (910, 4)\n",
      "task: text classification\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.41      0.47       572\n",
      "           1       0.25      0.38      0.30       250\n",
      "           2       0.06      0.04      0.05        46\n",
      "           3       0.06      0.12      0.08        42\n",
      "\n",
      "    accuracy                           0.37       910\n",
      "   macro avg       0.23      0.24      0.23       910\n",
      "weighted avg       0.43      0.37      0.39       910\n",
      "\n",
      "\n",
      "Fold  3\n",
      "language: en\n",
      "Word Counts: 6573\n",
      "Nrows: 7275\n",
      "7275 train sequences\n",
      "train sequence lengths:\n",
      "\tmean : 4\n",
      "\t95percentile : 10\n",
      "\t99percentile : 24\n",
      "x_train shape: (7275,150)\n",
      "y_train shape: (7275, 4)\n",
      "Is Multi-Label? False\n",
      "2046 test sequences\n",
      "test sequence lengths:\n",
      "\tmean : 6\n",
      "\t95percentile : 15\n",
      "\t99percentile : 66\n",
      "x_test shape: (2046,150)\n",
      "y_test shape: (2046, 4)\n",
      "task: text classification\n",
      "Is Multi-Label? False\n",
      "compiling word ID features...\n",
      "maxlen is 150\n",
      "building document-term matrix... this may take a few moments...\n",
      "rows: 1-7275\n",
      "computing log-count ratios...\n",
      "done.\n",
      "early_stopping automatically enabled at patience=5\n",
      "reduce_on_plateau automatically enabled at patience=2\n",
      "\n",
      "\n",
      "begin training using triangular learning rate policy with max lr of 0.01...\n",
      "Epoch 1/1024\n",
      "728/728 [==============================] - 2s 3ms/step - loss: 1.1791 - accuracy: 0.5342 - val_loss: 1.1888 - val_accuracy: 0.5127\n",
      "Epoch 2/1024\n",
      "728/728 [==============================] - 2s 3ms/step - loss: 0.8938 - accuracy: 0.6200 - val_loss: 1.2098 - val_accuracy: 0.5147\n",
      "Epoch 3/1024\n",
      "728/728 [==============================] - 2s 3ms/step - loss: 0.8551 - accuracy: 0.6179 - val_loss: 1.2523 - val_accuracy: 0.5098\n",
      "\n",
      "Epoch 00003: Reducing Max LR on Plateau: new max lr will be 0.005 (if not early_stopping).\n",
      "Epoch 4/1024\n",
      "728/728 [==============================] - 2s 3ms/step - loss: 0.8206 - accuracy: 0.6264 - val_loss: 1.2656 - val_accuracy: 0.5112\n",
      "Epoch 5/1024\n",
      "728/728 [==============================] - 2s 3ms/step - loss: 0.8159 - accuracy: 0.6347 - val_loss: 1.2884 - val_accuracy: 0.5112\n",
      "\n",
      "Epoch 00005: Reducing Max LR on Plateau: new max lr will be 0.0025 (if not early_stopping).\n",
      "Epoch 6/1024\n",
      "728/728 [==============================] - 2s 3ms/step - loss: 0.8157 - accuracy: 0.6316 - val_loss: 1.3052 - val_accuracy: 0.5108\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Weights from best epoch have been loaded into model.\n",
      "language: en\n",
      "Word Counts: 2214\n",
      "Nrows: 910\n",
      "910 train sequences\n",
      "train sequence lengths:\n",
      "\tmean : 5\n",
      "\t95percentile : 10\n",
      "\t99percentile : 31\n",
      "x_train shape: (910,150)\n",
      "y_train shape: (910, 4)\n",
      "Is Multi-Label? False\n",
      "910 test sequences\n",
      "test sequence lengths:\n",
      "\tmean : 5\n",
      "\t95percentile : 10\n",
      "\t99percentile : 31\n",
      "x_test shape: (910,150)\n",
      "y_test shape: (910, 4)\n",
      "task: text classification\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.44      0.50       585\n",
      "           1       0.21      0.29      0.25       235\n",
      "           2       0.07      0.07      0.07        46\n",
      "           3       0.07      0.14      0.09        44\n",
      "\n",
      "    accuracy                           0.37       910\n",
      "   macro avg       0.23      0.23      0.23       910\n",
      "weighted avg       0.43      0.37      0.39       910\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold  4\n",
      "language: en\n",
      "Word Counts: 7130\n",
      "Nrows: 7275\n",
      "7275 train sequences\n",
      "train sequence lengths:\n",
      "\tmean : 5\n",
      "\t95percentile : 12\n",
      "\t99percentile : 41\n",
      "x_train shape: (7275,150)\n",
      "y_train shape: (7275, 4)\n",
      "Is Multi-Label? False\n",
      "2046 test sequences\n",
      "test sequence lengths:\n",
      "\tmean : 4\n",
      "\t95percentile : 7\n",
      "\t99percentile : 26\n",
      "x_test shape: (2046,150)\n",
      "y_test shape: (2046, 4)\n",
      "task: text classification\n",
      "Is Multi-Label? False\n",
      "compiling word ID features...\n",
      "maxlen is 150\n",
      "building document-term matrix... this may take a few moments...\n",
      "rows: 1-7275\n",
      "computing log-count ratios...\n",
      "done.\n",
      "early_stopping automatically enabled at patience=5\n",
      "reduce_on_plateau automatically enabled at patience=2\n",
      "\n",
      "\n",
      "begin training using triangular learning rate policy with max lr of 0.01...\n",
      "Epoch 1/1024\n",
      "728/728 [==============================] - 2s 3ms/step - loss: 1.1691 - accuracy: 0.5215 - val_loss: 1.1103 - val_accuracy: 0.5411\n",
      "Epoch 2/1024\n",
      "728/728 [==============================] - 3s 4ms/step - loss: 0.9150 - accuracy: 0.6079 - val_loss: 1.0838 - val_accuracy: 0.5464\n",
      "Epoch 3/1024\n",
      "728/728 [==============================] - 3s 4ms/step - loss: 0.8499 - accuracy: 0.6223 - val_loss: 1.0891 - val_accuracy: 0.5425\n",
      "Epoch 4/1024\n",
      "728/728 [==============================] - 3s 4ms/step - loss: 0.8431 - accuracy: 0.6257 - val_loss: 1.0963 - val_accuracy: 0.5406\n",
      "\n",
      "Epoch 00004: Reducing Max LR on Plateau: new max lr will be 0.005 (if not early_stopping).\n",
      "Epoch 5/1024\n",
      "728/728 [==============================] - 2s 3ms/step - loss: 0.8355 - accuracy: 0.6215 - val_loss: 1.0958 - val_accuracy: 0.5455\n",
      "Epoch 6/1024\n",
      "728/728 [==============================] - 3s 4ms/step - loss: 0.8224 - accuracy: 0.6267 - val_loss: 1.0936 - val_accuracy: 0.5503\n",
      "\n",
      "Epoch 00006: Reducing Max LR on Plateau: new max lr will be 0.0025 (if not early_stopping).\n",
      "Epoch 7/1024\n",
      "728/728 [==============================] - 2s 3ms/step - loss: 0.8210 - accuracy: 0.6302 - val_loss: 1.0951 - val_accuracy: 0.5494\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Weights from best epoch have been loaded into model.\n",
      "language: en\n",
      "Word Counts: 1949\n",
      "Nrows: 910\n",
      "910 train sequences\n",
      "train sequence lengths:\n",
      "\tmean : 5\n",
      "\t95percentile : 11\n",
      "\t99percentile : 58\n",
      "x_train shape: (910,150)\n",
      "y_train shape: (910, 4)\n",
      "Is Multi-Label? False\n",
      "910 test sequences\n",
      "test sequence lengths:\n",
      "\tmean : 5\n",
      "\t95percentile : 11\n",
      "\t99percentile : 58\n",
      "x_test shape: (910,150)\n",
      "y_test shape: (910, 4)\n",
      "task: text classification\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.44      0.49       587\n",
      "           1       0.14      0.21      0.17       237\n",
      "           2       0.05      0.06      0.05        36\n",
      "           3       0.06      0.06      0.06        50\n",
      "\n",
      "    accuracy                           0.34       910\n",
      "   macro avg       0.20      0.19      0.19       910\n",
      "weighted avg       0.40      0.34      0.36       910\n",
      "\n",
      "\n",
      "Fold  5\n",
      "language: en\n",
      "Word Counts: 7506\n",
      "Nrows: 7275\n",
      "7275 train sequences\n",
      "train sequence lengths:\n",
      "\tmean : 5\n",
      "\t95percentile : 11\n",
      "\t99percentile : 47\n",
      "x_train shape: (7275,150)\n",
      "y_train shape: (7275, 4)\n",
      "Is Multi-Label? False\n",
      "2046 test sequences\n",
      "test sequence lengths:\n",
      "\tmean : 3\n",
      "\t95percentile : 8\n",
      "\t99percentile : 17\n",
      "x_test shape: (2046,150)\n",
      "y_test shape: (2046, 4)\n",
      "task: text classification\n",
      "Is Multi-Label? False\n",
      "compiling word ID features...\n",
      "maxlen is 150\n",
      "building document-term matrix... this may take a few moments...\n",
      "rows: 1-7275\n",
      "computing log-count ratios...\n",
      "done.\n",
      "early_stopping automatically enabled at patience=5\n",
      "reduce_on_plateau automatically enabled at patience=2\n",
      "\n",
      "\n",
      "begin training using triangular learning rate policy with max lr of 0.01...\n",
      "Epoch 1/1024\n",
      "728/728 [==============================] - 3s 3ms/step - loss: 1.1770 - accuracy: 0.5394 - val_loss: 1.1510 - val_accuracy: 0.4971\n",
      "Epoch 2/1024\n",
      "728/728 [==============================] - 2s 3ms/step - loss: 0.9007 - accuracy: 0.6282 - val_loss: 1.1544 - val_accuracy: 0.5015\n",
      "Epoch 3/1024\n",
      "728/728 [==============================] - 2s 3ms/step - loss: 0.8461 - accuracy: 0.6425 - val_loss: 1.1748 - val_accuracy: 0.4966\n",
      "\n",
      "Epoch 00003: Reducing Max LR on Plateau: new max lr will be 0.005 (if not early_stopping).\n",
      "Epoch 4/1024\n",
      "728/728 [==============================] - 3s 4ms/step - loss: 0.8178 - accuracy: 0.6443 - val_loss: 1.1746 - val_accuracy: 0.4951\n",
      "Epoch 5/1024\n",
      "728/728 [==============================] - 3s 4ms/step - loss: 0.8151 - accuracy: 0.6440 - val_loss: 1.1815 - val_accuracy: 0.4971\n",
      "\n",
      "Epoch 00005: Reducing Max LR on Plateau: new max lr will be 0.0025 (if not early_stopping).\n",
      "Epoch 6/1024\n",
      "728/728 [==============================] - 3s 3ms/step - loss: 0.8017 - accuracy: 0.6540 - val_loss: 1.1815 - val_accuracy: 0.4990\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Weights from best epoch have been loaded into model.\n",
      "language: en\n",
      "Word Counts: 2237\n",
      "Nrows: 910\n",
      "910 train sequences\n",
      "train sequence lengths:\n",
      "\tmean : 5\n",
      "\t95percentile : 11\n",
      "\t99percentile : 58\n",
      "x_train shape: (910,150)\n",
      "y_train shape: (910, 4)\n",
      "Is Multi-Label? False\n",
      "910 test sequences\n",
      "test sequence lengths:\n",
      "\tmean : 5\n",
      "\t95percentile : 11\n",
      "\t99percentile : 58\n",
      "x_test shape: (910,150)\n",
      "y_test shape: (910, 4)\n",
      "task: text classification\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.73      0.71       599\n",
      "           1       0.29      0.26      0.27       221\n",
      "           2       0.00      0.00      0.00        53\n",
      "           3       0.07      0.08      0.08        37\n",
      "\n",
      "    accuracy                           0.55       910\n",
      "   macro avg       0.26      0.27      0.27       910\n",
      "weighted avg       0.53      0.55      0.54       910\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "count = 1\n",
    "\n",
    "for train_index, test_index in kf.split(word_seq_train):\n",
    "    x_trn, x_tst = word_seq_train[train_index], word_seq_train[test_index]\n",
    "    y_trn, y_tst = y_train_initial[train_index], y_train_initial[test_index]\n",
    "    \n",
    "    x_new_train, x_val, y_new_train, y_val= train_test_split(x_trn, y_trn, test_size=0.11115, random_state=125)\n",
    "    \n",
    "    print(\"\\nFold \", count)\n",
    "    \n",
    "    (x_train,  y_train), (x_test, y_test),  preproc = text.texts_from_array(x_train=x_new_train, y_train=y_new_train,\n",
    "                                                                      x_test=x_tst, y_test=y_tst,\n",
    "                                                                       class_names=class_names,\n",
    "                                                                       preprocess_mode='standard',\n",
    "                                                                       maxlen=150, \n",
    "                                                                       max_features=1000)\n",
    "    \n",
    "    model = text.text_classifier('nbsvm', train_data=(x_train, y_train), preproc=preproc)\n",
    "    \n",
    "    learner = ktrain.get_learner(model, train_data=(x_train, y_train), \n",
    "                             val_data=(x_test, y_test),\n",
    "                             batch_size=10)\n",
    "    \n",
    "    \n",
    "    callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1)\n",
    "    #learner.lr_find()\n",
    "\n",
    "    \n",
    "    #learner.fit_onecycle(2e-5, 40, callbacks=[callbacks])\n",
    "\n",
    "    learner.autofit(0.01)\n",
    "\n",
    "    \n",
    "    #predictor = ktrain.get_predictor(learner.model, preproc)\n",
    "    #predictor.get_classes()\n",
    "    \n",
    "    (x_train,  y_train), (x_test, y_test),  preproc = text.texts_from_array(x_train=x_val, y_train=y_val,\n",
    "                                                                       x_test=x_val, y_test=y_val,\n",
    "                                                                       \n",
    "                                                                       class_names=class_names,\n",
    "                                                                       preprocess_mode='standard',\n",
    "                                                                       maxlen=150, \n",
    "                                                                       max_features=1000)\n",
    "    \n",
    "    learner.validate(val_data=(x_test, y_test), class_names=class_names)\n",
    "    \n",
    "    '''\n",
    "    print(metrics.classification_report(y_tst, y_pred))\n",
    "    \n",
    "    precision = precision_score(y_tst, y_pred, pos_label=1)\n",
    "    recall = recall_score(y_tst, y_pred, pos_label=1)\n",
    "    f1score = f1_score(y_tst, y_pred, pos_label=1)\n",
    "    lstm_accuracy = accuracy_score(y_tst, y_pred)\n",
    "\n",
    "    run_accuracy.append(accuracy)\n",
    "    run_f1score.append(f1score)\n",
    "    run_precision.append(precision)\n",
    "    run_recall.append(recall)\n",
    "    '''\n",
    "    count = count+1\n",
    "    #X_train = data['sourceBeforeFix'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8ec683",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ee91f77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bcafaaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a439476",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc6056a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
